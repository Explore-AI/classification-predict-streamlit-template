{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd               #For data loading and data manipulation\n",
    "import seaborn as sns             #For data visualization \n",
    "import matplotlib.pyplot as plt   #For data visualization\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #For stopword removals\n",
    "\n",
    "import string                     #For Punctuations\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display.max_colwidth option  set to None to show all text content without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Your code to read the DataFrame\n",
    "df1 = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test_with_no_labels.csv')\n",
    "df=pd.concat([df1,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It's not like we lack evidence of anthropogenic global warming</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year in the war on climate change https://t.co/44wOTxTLcD</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, sexist, climate change denying bigot is leading in the polls. #ElectionNight</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Worth a read whether you do or don't believe in climate change https://t.co/ggLZVNYjun https://t.co/7AFE2mAH8j</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @thenation: Mike Pence doesn’t believe in global warming or that smoking causes lung cancer. https://t.co/gvWYaauU8R</td>\n",
       "      <td>294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @makeandmendlife: Six big things we can ALL do today to fight climate change, or how to be a climate activistÃ¢â‚¬Â¦ https://t.co/TYMLu6DbNM hÃ¢â‚¬Â¦</td>\n",
       "      <td>992717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@AceofSpadesHQ My 8yo nephew is inconsolable. He wants to die of old age like me, but will perish in the fiery hellscape of climate change.</td>\n",
       "      <td>664510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @paigetweedy: no offense… but like… how do you just not believe… in global warming………</td>\n",
       "      <td>260471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0        1.0   \n",
       "1        1.0   \n",
       "2        2.0   \n",
       "3        1.0   \n",
       "4        1.0   \n",
       "5        1.0   \n",
       "6        1.0   \n",
       "7        1.0   \n",
       "8        1.0   \n",
       "9        1.0   \n",
       "\n",
       "                                                                                                                                                    message  \\\n",
       "0              PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable   \n",
       "1                                                                                            It's not like we lack evidence of anthropogenic global warming   \n",
       "2              RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…   \n",
       "3                                                       #TodayinMaker# WIRED : 2016 was a pivotal year in the war on climate change https://t.co/44wOTxTLcD   \n",
       "4                                RT @SoyNovioDeTodas: It's 2016, and a racist, sexist, climate change denying bigot is leading in the polls. #ElectionNight   \n",
       "5                                            Worth a read whether you do or don't believe in climate change https://t.co/ggLZVNYjun https://t.co/7AFE2mAH8j   \n",
       "6                                   RT @thenation: Mike Pence doesn’t believe in global warming or that smoking causes lung cancer. https://t.co/gvWYaauU8R   \n",
       "7  RT @makeandmendlife: Six big things we can ALL do today to fight climate change, or how to be a climate activistÃ¢â‚¬Â¦ https://t.co/TYMLu6DbNM hÃ¢â‚¬Â¦   \n",
       "8               @AceofSpadesHQ My 8yo nephew is inconsolable. He wants to die of old age like me, but will perish in the fiery hellscape of climate change.   \n",
       "9                                                                  RT @paigetweedy: no offense… but like… how do you just not believe… in global warming………   \n",
       "\n",
       "   tweetid  \n",
       "0   625221  \n",
       "1   126103  \n",
       "2   698562  \n",
       "3   573736  \n",
       "4   466954  \n",
       "5   425577  \n",
       "6   294933  \n",
       "7   992717  \n",
       "8   664510  \n",
       "9   260471  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data snippet shown above, we see that the message column contains some noise (stop words, puntuations, mentions, hashtags and even urls). We need to deal with the noise before proceeding to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26365, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26365 entries, 0 to 10545\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   sentiment  15819 non-null  float64\n",
      " 1   message    26365 non-null  object \n",
      " 2   tweetid    26365 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 823.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() #Let's see our datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a barg graph to show the distribution of the sentiments\n",
    "df['sentiment'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy() #Let us make a copy of our dataframe to avoid modifying the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#To get a list of english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable</td>\n",
       "      <td>625221</td>\n",
       "      <td>PolySciMajor EPA chief think carbon dioxide main cause global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>It's not like we lack evidence of anthropogenic global warming</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…</td>\n",
       "      <td>698562</td>\n",
       "      <td>RT @RawStory: Researchers say three years act climate change it’s late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year in the war on climate change https://t.co/44wOTxTLcD</td>\n",
       "      <td>573736</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 pivotal year war climate change https://t.co/44wOTxTLcD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, sexist, climate change denying bigot is leading in the polls. #ElectionNight</td>\n",
       "      <td>466954</td>\n",
       "      <td>RT @SoyNovioDeTodas: 2016, racist, sexist, climate change denying bigot leading polls. #ElectionNight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0        1.0   \n",
       "1        1.0   \n",
       "2        2.0   \n",
       "3        1.0   \n",
       "4        1.0   \n",
       "\n",
       "                                                                                                                                        message  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable   \n",
       "1                                                                                It's not like we lack evidence of anthropogenic global warming   \n",
       "2  RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…   \n",
       "3                                           #TodayinMaker# WIRED : 2016 was a pivotal year in the war on climate change https://t.co/44wOTxTLcD   \n",
       "4                    RT @SoyNovioDeTodas: It's 2016, and a racist, sexist, climate change denying bigot is leading in the polls. #ElectionNight   \n",
       "\n",
       "   tweetid  \\\n",
       "0   625221   \n",
       "1   126103   \n",
       "2   698562   \n",
       "3   573736   \n",
       "4   466954   \n",
       "\n",
       "                                                                                                                    clean_message  \n",
       "0  PolySciMajor EPA chief think carbon dioxide main cause global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable  \n",
       "1                                                                                 like lack evidence anthropogenic global warming  \n",
       "2             RT @RawStory: Researchers say three years act climate change it’s late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…  \n",
       "3                                             #TodayinMaker# WIRED : 2016 pivotal year war climate change https://t.co/44wOTxTLcD  \n",
       "4                           RT @SoyNovioDeTodas: 2016, racist, sexist, climate change denying bigot leading polls. #ElectionNight  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " clean_texts = []\n",
    "\n",
    "for text in df_train['message']:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    clean_texts.append(clean_text)\n",
    "\n",
    "# clean_texts\n",
    "df_train['clean_message'] = clean_texts\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "Should we drop the original message column at this point? Or would it be useful for future reference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Urls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove urls and replace with 'url'\n",
    " \n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "\n",
    "subs_url = r'url'\n",
    "\n",
    "df_train['clean_message'] = df_train['clean_message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @RawStory: Researchers say three years act climate change it’s late url url…'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['clean_message'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Convert message to lower case\n",
    "df_train['clean_message'] = df_train['clean_message'].str.lower()\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt paigetweedy offense… like… believe… global warming………'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(message):\n",
    "    return ''.join([l for l in message if l not in string.punctuation])\n",
    "\n",
    "\n",
    "df_train['clean_message'] = df_train['clean_message'].apply(remove_punctuation)\n",
    "df_train['clean_message'].iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt rawstory researchers say three years act climate change it’s late url url…'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['clean_message'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite removing puntuations, we still observe some curly quotation marks and repeated punctuations.\n",
    "\n",
    "Let's deal with these using regular expression(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt rawstory researchers say three years act climate change its late url url'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s\"“”]', '', text)  # Remove non-alphanumeric characters and curly quotation marks\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
    "    text = re.sub(r'([!?.])\\1+', r'\\1', text)  # Remove repeated punctuation marks\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the clean_text function to the 'message' column\n",
    "df_train['clean_message'] = df_train['clean_message'].apply(clean_text)\n",
    "df_train['clean_message'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt paigetweedy offense like believe global warming'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['clean_message'].iloc[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our data has been delivered of every noise......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26365, 4)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.drop([\"message\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researchers say three years act climate change its late url url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war climate change url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  tweetid  \\\n",
       "0        1.0   625221   \n",
       "1        1.0   126103   \n",
       "2        2.0   698562   \n",
       "3        1.0   573736   \n",
       "4        1.0   466954   \n",
       "\n",
       "                                                                                          clean_message  \n",
       "0  polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable  \n",
       "1                                                       like lack evidence anthropogenic global warming  \n",
       "2                           rt rawstory researchers say three years act climate change its late url url  \n",
       "3                                           todayinmaker wired 2016 pivotal year war climate change url  \n",
       "4        rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(df_train['clean_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researchers say three years act climate change its late url url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war climate change url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  tweetid  \\\n",
       "0        1.0   625221   \n",
       "1        1.0   126103   \n",
       "2        2.0   698562   \n",
       "3        1.0   573736   \n",
       "4        1.0   466954   \n",
       "\n",
       "                                                                                          clean_message  \n",
       "0  polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable  \n",
       "1                                                       like lack evidence anthropogenic global warming  \n",
       "2                           rt rawstory researchers say three years act climate change its late url url  \n",
       "3                                           todayinmaker wired 2016 pivotal year war climate change url  \n",
       "4        rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=betterVect.fit_transform(df_train['clean_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.5, min_df=2, stop_words='english')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researchers say three years act climate change its late url url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war climate change url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>NaN</td>\n",
       "      <td>895714</td>\n",
       "      <td>rt brittanybohrer brb writing poem climate change climatechange science poetry fakenews alternativefacts url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>NaN</td>\n",
       "      <td>875167</td>\n",
       "      <td>2016 year climate change came home hottest year record karl mathiesen travelled tasmania url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>NaN</td>\n",
       "      <td>78329</td>\n",
       "      <td>rt loopvanuatu pacific countries positive fiji leading global climate change conference november url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>NaN</td>\n",
       "      <td>867455</td>\n",
       "      <td>rt xanria00018 youre hot must cause global warming aldublaboroflove jophie30 asn585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>NaN</td>\n",
       "      <td>470892</td>\n",
       "      <td>rt chloebalaoing climate change global issue thats getting worse eating plant based least amount effort h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26365 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment  tweetid  \\\n",
       "0            1.0   625221   \n",
       "1            1.0   126103   \n",
       "2            2.0   698562   \n",
       "3            1.0   573736   \n",
       "4            1.0   466954   \n",
       "...          ...      ...   \n",
       "10541        NaN   895714   \n",
       "10542        NaN   875167   \n",
       "10543        NaN    78329   \n",
       "10544        NaN   867455   \n",
       "10545        NaN   470892   \n",
       "\n",
       "                                                                                                      clean_message  \n",
       "0              polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable  \n",
       "1                                                                   like lack evidence anthropogenic global warming  \n",
       "2                                       rt rawstory researchers say three years act climate change its late url url  \n",
       "3                                                       todayinmaker wired 2016 pivotal year war climate change url  \n",
       "4                    rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight  \n",
       "...                                                                                                             ...  \n",
       "10541  rt brittanybohrer brb writing poem climate change climatechange science poetry fakenews alternativefacts url  \n",
       "10542                  2016 year climate change came home hottest year record karl mathiesen travelled tasmania url  \n",
       "10543          rt loopvanuatu pacific countries positive fiji leading global climate change conference november url  \n",
       "10544                           rt xanria00018 youre hot must cause global warming aldublaboroflove jophie30 asn585  \n",
       "10545     rt chloebalaoing climate change global issue thats getting worse eating plant based least amount effort h  \n",
       "\n",
       "[26365 rows x 3 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4009)\t1\n",
      "  (0, 2133)\t1\n",
      "  (0, 11177)\t1\n",
      "  (0, 1865)\t1\n",
      "  (0, 3363)\t1\n",
      "  (0, 6908)\t1\n",
      "  (0, 1934)\t1\n",
      "  (0, 4929)\t1\n",
      "  (0, 12007)\t1\n",
      "  (0, 11972)\t1\n",
      "  (0, 11771)\t1\n",
      "  (0, 7033)\t1\n",
      "  (1, 4929)\t1\n",
      "  (1, 12007)\t1\n",
      "  (1, 6703)\t1\n",
      "  (1, 6479)\t1\n",
      "  (1, 4112)\t1\n",
      "  (1, 848)\t1\n",
      "  (2, 11771)\t2\n",
      "  (2, 9093)\t1\n",
      "  (2, 9398)\t1\n",
      "  (2, 9791)\t1\n",
      "  (2, 12405)\t1\n",
      "  (2, 393)\t1\n",
      "  (2, 6524)\t1\n",
      "  :\t:\n",
      "  (26362, 6570)\t1\n",
      "  (26362, 2763)\t1\n",
      "  (26362, 7777)\t1\n",
      "  (26362, 2562)\t1\n",
      "  (26362, 8096)\t1\n",
      "  (26362, 8601)\t1\n",
      "  (26362, 4445)\t1\n",
      "  (26363, 1934)\t1\n",
      "  (26363, 4929)\t1\n",
      "  (26363, 12007)\t1\n",
      "  (26363, 5526)\t1\n",
      "  (26363, 12436)\t1\n",
      "  (26363, 627)\t1\n",
      "  (26363, 12366)\t1\n",
      "  (26363, 6229)\t1\n",
      "  (26363, 1013)\t1\n",
      "  (26364, 4929)\t1\n",
      "  (26364, 11076)\t1\n",
      "  (26364, 6040)\t1\n",
      "  (26364, 3737)\t1\n",
      "  (26364, 12310)\t1\n",
      "  (26364, 8445)\t1\n",
      "  (26364, 3805)\t1\n",
      "  (26364, 4887)\t1\n",
      "  (26364, 1238)\t1\n"
     ]
    }
   ],
   "source": [
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  2., ..., nan, nan, nan])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment\n",
       "0            1.0\n",
       "1            1.0\n",
       "2            2.0\n",
       "3            1.0\n",
       "4            1.0\n",
       "...          ...\n",
       "15814        1.0\n",
       "15815        2.0\n",
       "15816        0.0\n",
       "15817       -1.0\n",
       "15818        0.0\n",
       "\n",
       "[15819 rows x 1 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df_train[:len(df1)][[\"sentiment\"]]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4009)\t1\n",
      "  (0, 2133)\t1\n",
      "  (0, 11177)\t1\n",
      "  (0, 1865)\t1\n",
      "  (0, 3363)\t1\n",
      "  (0, 6908)\t1\n",
      "  (0, 1934)\t1\n",
      "  (0, 4929)\t1\n",
      "  (0, 12007)\t1\n",
      "  (0, 11972)\t1\n",
      "  (0, 11771)\t1\n",
      "  (0, 7033)\t1\n",
      "  (1, 4929)\t1\n",
      "  (1, 12007)\t1\n",
      "  (1, 6703)\t1\n",
      "  (1, 6479)\t1\n",
      "  (1, 4112)\t1\n",
      "  (1, 848)\t1\n",
      "  (2, 11771)\t2\n",
      "  (2, 9093)\t1\n",
      "  (2, 9398)\t1\n",
      "  (2, 9791)\t1\n",
      "  (2, 12405)\t1\n",
      "  (2, 393)\t1\n",
      "  (2, 6524)\t1\n",
      "  :\t:\n",
      "  (15815, 11771)\t1\n",
      "  (15815, 1624)\t1\n",
      "  (15815, 12032)\t1\n",
      "  (15815, 9290)\t1\n",
      "  (15815, 91)\t1\n",
      "  (15816, 11771)\t1\n",
      "  (15816, 1330)\t1\n",
      "  (15816, 11480)\t1\n",
      "  (15816, 9520)\t1\n",
      "  (15816, 8308)\t1\n",
      "  (15816, 6202)\t1\n",
      "  (15816, 581)\t1\n",
      "  (15816, 422)\t1\n",
      "  (15816, 7837)\t1\n",
      "  (15817, 5389)\t1\n",
      "  (15817, 2037)\t1\n",
      "  (15817, 5460)\t1\n",
      "  (15817, 6674)\t1\n",
      "  (15817, 2811)\t1\n",
      "  (15817, 11250)\t1\n",
      "  (15817, 546)\t1\n",
      "  (15817, 9753)\t1\n",
      "  (15817, 1778)\t1\n",
      "  (15818, 11771)\t1\n",
      "  (15818, 4024)\t1\n"
     ]
    }
   ],
   "source": [
    "X_train=df_new[:len(df1)]\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  2., ...,  0., -1.,  0.])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST=df_new[len(df1):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our parameters are y,X_train,X_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train,y[\"sentiment\"].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA NORMALIZATION\n",
    "\n",
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "df_train['tokens'] = df_train['clean_message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'rawstory',\n",
       " 'researchers',\n",
       " 'say',\n",
       " 'three',\n",
       " 'years',\n",
       " 'act',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'its',\n",
       " 'late',\n",
       " 'url',\n",
       " 'url']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tokens'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable</td>\n",
       "      <td>[polyscimajor, epa, chief, think, carbon, dioxide, main, cause, global, warming, and, wait, what, url, via, mashable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, warming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researchers say three years act climate change its late url url</td>\n",
       "      <td>[rt, rawstory, researchers, say, three, years, act, climate, change, its, late, url, url]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war climate change url</td>\n",
       "      <td>[todayinmaker, wired, 2016, pivotal, year, war, climate, change, url]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight</td>\n",
       "      <td>[rt, soynoviodetodas, 2016, racist, sexist, climate, change, denying, bigot, leading, polls, electionnight]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  tweetid  \\\n",
       "0        1.0   625221   \n",
       "1        1.0   126103   \n",
       "2        2.0   698562   \n",
       "3        1.0   573736   \n",
       "4        1.0   466954   \n",
       "\n",
       "                                                                                          clean_message  \\\n",
       "0  polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable   \n",
       "1                                                       like lack evidence anthropogenic global warming   \n",
       "2                           rt rawstory researchers say three years act climate change its late url url   \n",
       "3                                           todayinmaker wired 2016 pivotal year war climate change url   \n",
       "4        rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight   \n",
       "\n",
       "                                                                                                                  tokens  \n",
       "0  [polyscimajor, epa, chief, think, carbon, dioxide, main, cause, global, warming, and, wait, what, url, via, mashable]  \n",
       "1                                                                 [like, lack, evidence, anthropogenic, global, warming]  \n",
       "2                              [rt, rawstory, researchers, say, three, years, act, climate, change, its, late, url, url]  \n",
       "3                                                  [todayinmaker, wired, 2016, pivotal, year, war, climate, change, url]  \n",
       "4            [rt, soynoviodetodas, 2016, racist, sexist, climate, change, denying, bigot, leading, polls, electionnight]  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stem'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'stem'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [231]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Scale the tweet vectors\u001b[39;00m\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler(with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'stem'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the tweet vectors\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_scaled = scaler.fit_transform(df_train['stem'].values)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "#X_TEST_scaled= scaler.transform(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable</td>\n",
       "      <td>[polyscimajor, epa, chief, think, carbon, dioxide, main, cause, global, warming, and, wait, what, url, via, mashable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, warming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researchers say three years act climate change its late url url</td>\n",
       "      <td>[rt, rawstory, researchers, say, three, years, act, climate, change, its, late, url, url]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired 2016 pivotal year war climate change url</td>\n",
       "      <td>[todayinmaker, wired, 2016, pivotal, year, war, climate, change, url]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight</td>\n",
       "      <td>[rt, soynoviodetodas, 2016, racist, sexist, climate, change, denying, bigot, leading, polls, electionnight]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  tweetid  \\\n",
       "0        1.0   625221   \n",
       "1        1.0   126103   \n",
       "2        2.0   698562   \n",
       "3        1.0   573736   \n",
       "4        1.0   466954   \n",
       "\n",
       "                                                                                          clean_message  \\\n",
       "0  polyscimajor epa chief think carbon dioxide main cause global warming and wait what url via mashable   \n",
       "1                                                       like lack evidence anthropogenic global warming   \n",
       "2                           rt rawstory researchers say three years act climate change its late url url   \n",
       "3                                           todayinmaker wired 2016 pivotal year war climate change url   \n",
       "4        rt soynoviodetodas 2016 racist sexist climate change denying bigot leading polls electionnight   \n",
       "\n",
       "                                                                                                                  tokens  \n",
       "0  [polyscimajor, epa, chief, think, carbon, dioxide, main, cause, global, warming, and, wait, what, url, via, mashable]  \n",
       "1                                                                 [like, lack, evidence, anthropogenic, global, warming]  \n",
       "2                              [rt, rawstory, researchers, say, three, years, act, climate, change, its, late, url, url]  \n",
       "3                                                  [todayinmaker, wired, 2016, pivotal, year, war, climate, change, url]  \n",
       "4            [rt, soynoviodetodas, 2016, racist, sexist, climate, change, denying, bigot, leading, polls, electionnight]  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the SVC is: 0.7149178255372945\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.59      0.49      0.54       278\n",
      "         0.0       0.44      0.54      0.48       425\n",
      "         1.0       0.80      0.77      0.79      1755\n",
      "         2.0       0.76      0.76      0.76       706\n",
      "\n",
      "    accuracy                           0.71      3164\n",
      "   macro avg       0.65      0.64      0.64      3164\n",
      "weighted avg       0.72      0.71      0.72      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the SVC is: 0.6014538558786346\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.40      0.52      0.45       278\n",
      "         0.0       0.34      0.50      0.41       425\n",
      "         1.0       0.76      0.64      0.69      1755\n",
      "         2.0       0.60      0.61      0.61       706\n",
      "\n",
      "    accuracy                           0.60      3164\n",
      "   macro avg       0.53      0.57      0.54      3164\n",
      "weighted avg       0.64      0.60      0.61      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was noticed that training with unscaled data led to higher training accuracy but lower test accuracy. Inshort scale ur data. Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch Using SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svc__C': 10, 'svc__kernel': 'rbf'}\n",
      "Best Score: 0.679494271039115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Create a pipeline with CountVectorizer and SVC\n",
    "vectorizer = CountVectorizer()\n",
    "pipeline = Pipeline([\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV on the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_train_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr.fit(X_train_scaled, y_train)\n",
    "#Apply stemming function\n",
    "df_train['stem'] = df_train['tokens'].apply(df_train_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    0: Neutral       0.53      0.46      0.50       278\n",
      " 1: ProClimate       0.40      0.51      0.45       425\n",
      "       2: News       0.76      0.72      0.74      1755\n",
      "-1:AntiClimate       0.67      0.67      0.67       706\n",
      "\n",
      "      accuracy                           0.66      3164\n",
      "     macro avg       0.59      0.59      0.59      3164\n",
      "  weighted avg       0.67      0.66      0.66      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lr, target_names=['0: Neutral', '1: ProClimate','2: News','-1:AntiClimate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6463337547408344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "nb_classifier = MultinomialNB()\n",
    "df_train['stem'] = df_train['tokens'].apply(df_train_stemmer, args=(stemmer, ))\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = nb_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.6855246523388117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "mlp_predictions = mlp_classifier.predict(X_test_scaled)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "print(\"MLP Accuracy:\", mlp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Assuming you have already prepared your training and testing data: X_trainrnn, X_testrnn, y_train, y_test\n",
    "\n",
    "# Convert sparse input data to dense arrays\n",
    "X_trainrnn = X_train_scaled.toarray()\n",
    "X_testrnn = X_test_scaled.toarray()\n",
    "\n",
    "# Determine the number of unique tokens in your input data\n",
    "num_tokens = int(np.max([np.max(X_trainrnn[i, :]) for i in range(X_trainrnn.shape[0])]) + 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_tokens, output_dim=32))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_trainrnn, y_train, batch_size=8, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(X_testrnn, y_test)\n",
    "print(\"RNN Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Assuming X_train_scaled, X_test_scaled, y_train, and y_test are available\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert input data to list of strings\n",
    "X_train_texts = [str(seq) for seq in X_train_scaled]\n",
    "X_test_texts = [str(seq) for seq in X_test_scaled]\n",
    "\n",
    "# Tokenize and pad the input sequences\n",
    "input_ids = tokenizer.batch_encode_plus(X_train_texts, padding='longest', truncation=True, return_tensors='tf')['input_ids']\n",
    "input_ids_test = tokenizer.batch_encode_plus(X_test_texts, padding='longest', truncation=True, return_tensors='tf')['input_ids']\n",
    "\n",
    "# Build the model architecture\n",
    "input_layer = Input(shape=(input_ids.shape[1],), dtype=tf.int32)\n",
    "bert_output = bert_model(input_layer)[0][:, 0, :]\n",
    "output_layer = Dense(1, activation='sigmoid')(bert_output)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_ids, y_train, batch_size=8, epochs=3, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(input_ids_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Transformers Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Random Forest Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_rf_classifier = RandomForestClassifier(**best_params)\n",
    "best_rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = best_rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Random Forest Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainint=[int(i) for i in y_train]\n",
    "y_testint=[int(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map the labels to [0, 1, 2, 3] instead of 0,-1,2,\n",
    "\n",
    "\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping_train = {label: idx for idx, label in enumerate(np.unique(y_trainint))}\n",
    "labels_train = np.array([label_mapping_train[label] for label in y_trainint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping_test = {label: idx for idx, label in enumerate(np.unique(y_testint))}\n",
    "labels_test = np.array([label_mapping_test[label] for label in y_testint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Initialize XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(X_train_scaled, labels_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_test = xgb_classifier.predict(X_test_scaled)\n",
    "\n",
    "#Now we convert changed prediction labels back\n",
    "\n",
    "# Define inverse mapping dictionaries\n",
    "inv_label_mapping_train = {idx: label for label, idx in label_mapping_train.items()}\n",
    "inv_label_mapping_test = {idx: label for label, idx in label_mapping_test.items()}\n",
    "y_pred_original_test = np.array([inv_label_mapping_test[pred] for pred in y_pred_test])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_original_test)\n",
    "print(\"XGBoost Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier using Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "grid_search = GridSearchCV(xgb_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, labels_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_xgb_classifier = xgb.XGBClassifier(**best_params)\n",
    "best_xgb_classifier.fit(X_train_scaled, labels_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_test = best_xgb_classifier.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "#Now we convert changed prediction labels back\n",
    "\n",
    "# Define inverse mapping dictionaries\n",
    "inv_label_mapping_train = {idx: label for label, idx in label_mapping_train.items()}\n",
    "inv_label_mapping_test = {idx: label for label, idx in label_mapping_test.items()}\n",
    "y_pred_original_test = np.array([inv_label_mapping_test[pred] for pred in y_pred_test])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_original_test)\n",
    "print(\"XGBoost Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"XGBoost Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=lr.predict(X_TEST_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[int(i) for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAF=pd.DataFrame(result,columns=[\"sentiment\"])\n",
    "DAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, result)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame({\"tweetid\":test_df[\"tweetid\"]})\n",
    "submission=output.join(DAF)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submissions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "df_train['tokens'] = df_train['clean_message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGNORE THE SECTIONS BELOW THIS CELL FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'loving loves lovingly loved lover lovable'\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = [x for x in ngrams('Classification is a great machine learning technique to learn'.split(' '), 2)]\n",
    "print(len(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
