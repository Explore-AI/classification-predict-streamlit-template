{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUE6Op_LLx_G"
   },
   "source": [
    "# SMEND CONSULTS Sentiment Analysis on Climate Change Predict.\n",
    "© Explore Data Science Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eng32_bHMIeZ"
   },
   "source": [
    "# The Team\n",
    " 1. Sodiq Ambali - Team lead\n",
    " 2. Eteng Uket\n",
    " 3. Nichodemus Amollo\n",
    " 4. Michael Omosebi\n",
    " 5. Dorcas Solonka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjQJyhgjLIql"
   },
   "source": [
    "# Climate Change Belief Analysis 2022\n",
    "\n",
    "### Overview: Predict an individual’s belief in climate change based on historical tweet data.\n",
    "   \n",
    "- Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. \n",
    "\n",
    "## Problem Statement\n",
    "#### In this project we would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to companies market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "### Variable definitions\n",
    "- sentiment: Sentiment of tweet\n",
    "- message: Tweet body\n",
    "- tweetid: Twitter unique id\n",
    "\n",
    "### Sentiment of tweet Description\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "* -1 Anti: the tweet does not believe in man-made climate change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWJRKnp1LIq5"
   },
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFlux2e-LIq8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sKKVIF7OLIq_",
    "outputId": "b072a79b-ce18-4402-9fc0-f4da44bd1533"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/sodiq-olamide/climate-change-belief-analysis/32cdc371ce534f889cb53973094a67d3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import comet_ml at the top of your file\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"pZRIYy5yWLxOVTSX7gAbXSwXX\",\n",
    "    project_name=\"climate-change-belief-analysis\",\n",
    "    workspace=\"sodiq-olamide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvWaz0n2LIrI"
   },
   "source": [
    "#### For any Machine Learning projects certain python packages must be imported in order to carry out the task.\n",
    "### Below contains all package necessary for predicting individuals beliefs in climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloudNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\stella-pc\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "    Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\STELLA-PC\\Anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\STELLA-PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-c1hlgbpd\\\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\STELLA-PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-c1hlgbpd\\\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\STELLA-PC\\AppData\\Local\\Temp\\pip-wheel-2l3vhlvy'\n",
      "       cwd: C:\\Users\\STELLA-PC\\AppData\\Local\\Temp\\pip-install-c1hlgbpd\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\n",
      "  Complete output (20 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "  UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\STELLA-PC\\Anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\STELLA-PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-c1hlgbpd\\\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\STELLA-PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-c1hlgbpd\\\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\STELLA-PC\\AppData\\Local\\Temp\\pip-record-j6bdfg9w\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\STELLA-PC\\Anaconda3\\Include\\wordcloud'\n",
      "         cwd: C:\\Users\\STELLA-PC\\AppData\\Local\\Temp\\pip-install-c1hlgbpd\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\n",
      "    Complete output (20 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "    set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\STELLA-PC\\Anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\STELLA-PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-c1hlgbpd\\\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\STELLA-PC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-c1hlgbpd\\\\wordcloud_1f7de583812046d8b976230db6f64bf5\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\STELLA-PC\\AppData\\Local\\Temp\\pip-record-j6bdfg9w\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\STELLA-PC\\Anaconda3\\Include\\wordcloud' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_10244/992506014.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\STELLA~1\\AppData\\Local\\Temp/ipykernel_10244/992506014.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Download WordCloud()\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Download WordCloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1886,
     "status": "ok",
     "timestamp": 1649237655008,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "AtvfyMPxLIrK"
   },
   "outputs": [],
   "source": [
    "#NumPy can be used to perform a wide variety of mathematical operations on arrays\n",
    "import numpy as np\n",
    "#Pandas is used for working with data sets.It has functions for analyzing, cleaning, exploring, and manipulating data.\n",
    "import pandas as pd\n",
    "from pandas import MultiIndex\n",
    "\n",
    "#Below are comprehensive libraries for creating static, animated, and interactive visualizations.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from plotly import graph_objects as go\n",
    "# set plot style\n",
    "sns.set()\n",
    "#Regular Expression used for data cleaning\n",
    "import re\n",
    "\n",
    "#Text processing packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Model evaluation packages\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score, precision_score,  recall_score\n",
    "\n",
    "#Packages to split the data for testing and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Packages for features selection\n",
    "#from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "#Modelling Packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#Imbalanced data processing packages\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZzIaHmFLIrQ"
   },
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we to load the data file's into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtaWGt0FLIrT"
   },
   "source": [
    " In predicting Individuals beliefs in climate change based on historical data, supervised classification techniques must be performed, here for we must load in the training dataset for training the classifier model's and test dataset for predicting the result of the trained model's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1649237741039,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "BwLbWCmhLIrW"
   },
   "outputs": [],
   "source": [
    "#Load the tweet dataset into a dataframe\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWqiASW0LIrZ"
   },
   "source": [
    "The training dataset is stored in a variable named train_df, while the test dataset is stored in test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XLK_F8DLIrb"
   },
   "source": [
    "The next and one of the most important process is to perform an EDA, that is Exploratory Data Analysis, which involves analyzing the training dataset in other to derive meaningful insights about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVJAgrMMLIrc"
   },
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SViPrxJLIre"
   },
   "source": [
    "Getting a view of our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1649237745272,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "paomNZU8LIrf",
    "outputId": "75e0957b-751a-4228-f8fc-17073e5a5070"
   },
   "outputs": [],
   "source": [
    "#Viewing the dataframe.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzKdoBpeLIrg"
   },
   "source": [
    "Get feature name and their types - to check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1649237750158,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "FWC7Ln5sLIrh",
    "outputId": "bacda9b3-31ed-4a45-f251-db7a105382a6"
   },
   "outputs": [],
   "source": [
    "## getting feature name and their types - to check for missing values\n",
    "print(train_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS9KRdxoLIri"
   },
   "source": [
    "Both sentiment and tweet id are numerical columns with an Integer datatype. The message column is object type. There are 15819 values in each column, which confirms there is no missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc_4wADmLIri"
   },
   "source": [
    "#### Further check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1649237758057,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "brv1IiN7LIrj",
    "outputId": "b1d9300e-212c-4511-f6cf-dbb992699031"
   },
   "outputs": [],
   "source": [
    "## NULL values in the data\n",
    "print (\"Null Value Statistics:\", '\\n',train_df.isnull().sum()) ## Sum will tell the total number of NULL values inside the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q_JQrcqLIrk"
   },
   "source": [
    "The data is complete - no column has missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPj_Q73hLIrl"
   },
   "source": [
    "Get the shape of the data to understand the number of observations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649237760413,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "Y0asSnd7LIrl",
    "outputId": "882d3dd5-4189-4b92-9233-6de7503b4818"
   },
   "outputs": [],
   "source": [
    "## 6) shape of data\n",
    "print(\"Shape of Data : \\n\", train_df.shape, \"\\n\")\n",
    "print(\"No. of rows in the data = \", train_df.shape[0])\n",
    "print(\"No. of columns in the data = \", train_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4fARlkwLIrm"
   },
   "source": [
    "The historical train dataset contains 15819 rows and 3 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5Y7FZEELIrn"
   },
   "source": [
    "#### Distribution of tweets by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1649237765636,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "BshcTgDsLIro",
    "outputId": "a0e02c06-063e-4817-dff3-5a553ce8493d"
   },
   "outputs": [],
   "source": [
    "# see statistics for categorical features\n",
    "print(\"See distribution of messages per sentiment : \")\n",
    "count = train_df.groupby(\"sentiment\").count()[\"message\"].reset_index().sort_values(by=\"message\", ascending=False)\n",
    "count.style.background_gradient(cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "Sclass = ['Pro', 'News', 'Neutral', 'Anti']\n",
    "counts = [8530, 3640, 2353, 1296]\n",
    "ax.bar(Sclass,counts)\n",
    "plt.title('Tweets Sentiment Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmCP4P0xLIro"
   },
   "source": [
    "1 Pro tweets: the number of tweets that support the belief of man-made climate change 8530.\n",
    "\n",
    "2 News: the number of tweets that link to factual news about climate change are 3640\n",
    "\n",
    "0 Neutral: the number of tweets that neither support nor refute the belief of man-made climate change are 2353\n",
    "\n",
    "-1 Anti: the number of tweets that does not believe in man-made climate change are 1296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8D0htOlLIrp"
   },
   "source": [
    "Getting Statistics for the Non -Numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1649237769258,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "sge3i1jjLIrp",
    "outputId": "fb431558-75ba-4138-92d6-a85369d2f640"
   },
   "outputs": [],
   "source": [
    "## see statistics for non- numerical features\n",
    "print(\"See statistics for non- numerical columns : \")\n",
    "train_df.groupby('sentiment').describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D86_tknELIrq"
   },
   "source": [
    "Out of all tweets, there are a total of 14229 unique tweets. the most common tweet for Pro Tweet sentiment Class is \"RT @StephenSchlegel: she's thinking about how...\". it occurs 307 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKL0SNH7LIrq"
   },
   "source": [
    "Visualizing the distribution tweets by Sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1649237773243,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "uQzd-5xaLIrr",
    "outputId": "8c812d06-a5da-4147-9ac4-fa088f8fe556"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Funnel(\n",
    "    y = [\"Pro\",\"News\", \"Nuetral\", \"Anti\"],\n",
    "    x = train_df.sentiment.value_counts(normalize = True) * 100\n",
    "    ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IsvILNULIrs"
   },
   "source": [
    "Ohh we can see from the above plot that our training data contains an uneven distributions of data between the sentiment classes.\n",
    "\n",
    "Pro tweets occupy 53.92% of the total tweets.\n",
    "\n",
    "News tweets occupy 23.02% of the total tweets.\n",
    "\n",
    "Neutral tweets occupy 14.87% of the total tweets.\n",
    "\n",
    "Anti tweets occupy 8.19% of the total tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fegHv0ehLIrs"
   },
   "source": [
    "It clear that more than half of the tweets samples supports the beliefs of man-made climate change, while very few tweet sample does not believe in man-made climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmjYG6E_LIrs"
   },
   "source": [
    "Viewing an Entire length of message feature in the tweet dataframe, so as to find patterns on how most tweet messages are typed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1649237781894,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "bIj6x5LaLIrt",
    "outputId": "ed6ccde0-3141-4293-ab91-fb187c007bd3"
   },
   "outputs": [],
   "source": [
    "print('Show the full_text of 10 tweets: ')\n",
    "for tweet in train_df['message'][100:110]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuyShmI7LIru"
   },
   "source": [
    "Alright... Progress, we can now easily see a subset of full_text in every row of the train dataset column titled message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF7pHbdBLIru"
   },
   "source": [
    "Checking the tweetid column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1649237785257,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "pSdD9scVLIrv",
    "outputId": "36f7d209-d156-4bff-93b0-e9e970577410"
   },
   "outputs": [],
   "source": [
    "train_df['tweetid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhukEpPGLIrv"
   },
   "source": [
    "From the result above, each tweetid is unique, meaning it does not repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wISDXw83LIrw"
   },
   "source": [
    "#### Getting More Information through visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1649237790882,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "DLpEzjpYLIrw"
   },
   "outputs": [],
   "source": [
    "full_text = \" \".join(train_df['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmrLsIt4LIrw"
   },
   "source": [
    "We joined the entire message column in the train dataset, so has to be able to get the most reoccurring word or words in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ7H9huuLIrx"
   },
   "source": [
    "#### Visualizing the most reoccuring words with Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 3275,
     "status": "ok",
     "timestamp": 1649237798508,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "nvFMoogYLIrx",
    "outputId": "fccbde6d-5cd1-4383-c97c-859c2053f56a"
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white')\n",
    "img = wc.generate(full_text)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v1239IzLIrx"
   },
   "source": [
    "The Most repeated words in the tweet message are:\n",
    "\n",
    "    . Climate, Change, Global, Warming, Change, https, RT, Today, CO e.t.c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra0emlMoLIry"
   },
   "source": [
    "Now Lets get the Common words based on each tweet Sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_lp8YTjLIry"
   },
   "source": [
    "First we groupby tweets Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1649237803229,
     "user": {
      "displayName": "Solonka Sempeyian",
      "userId": "11821087901275073053"
     },
     "user_tz": -180
    },
    "id": "t2CyHbCtLIrz"
   },
   "outputs": [],
   "source": [
    "gb = train_df.groupby('sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi8oCflNLIrz"
   },
   "source": [
    "Now lets get sentiments from each groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH6ZpSbBLIr0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET ERROR: File could not be uploaded\n"
     ]
    }
   ],
   "source": [
    "Anti = \"\".join(gb.get_group(-1)['message'])\n",
    "Neutral = \"\".join(gb.get_group(0)['message'])\n",
    "Pro = \"\".join(gb.get_group(1)['message'])\n",
    "News = \"\".join(gb.get_group(2)['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_YSBUpKLIr0"
   },
   "source": [
    "ANTI Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khRX5XI4LIr1",
    "outputId": "89237c46-bd85-41ba-a78e-362d48faee52"
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black')\n",
    "img = wc.generate(Anti)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Most repeated words in the Anti Sentiment class are:\n",
    "\n",
    "climate, change, scientist, global, warming, e.t.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cLKf0iYLIr2"
   },
   "source": [
    "NEUTRAL Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bwn7VgIPLIr2",
    "outputId": "122b487b-7748-4242-e9f9-8b32fe334bef"
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black')\n",
    "img = wc.generate(Neutral)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Most repeated words in the Neutral Sentiment class are:\n",
    "\n",
    "climate, change, global, warming, e.t.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJgOeEkjLIr3"
   },
   "source": [
    "Pro Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaUirpK8LIr3",
    "outputId": "ed60d87e-0508-4c8e-9b40-f3cc00a305e9"
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black')\n",
    "img = wc.generate(Pro)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Most repeated words in the Pro Sentiment class are:\n",
    "\n",
    "climate, change, global, warming, e.t.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnGE5OQYLIr4"
   },
   "source": [
    "Factual News Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbW_abxtLIr4",
    "outputId": "75b15249-fc5f-4fe5-e7f1-8bd2c7297d13"
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black')\n",
    "img = wc.generate(News)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Most repeated words in the Factual News Sentiment class are:\n",
    "\n",
    "climate, change, global, warming, trump, e.t.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before finalizing the Exploratory data analysis, \n",
    "\n",
    "Lets see how sentiment class reacts to climate change through their tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anti Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(-1)['message'][100:105]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above, Anti Sentiment class mostly believes global climate change is not real, but a manipulatory agenda by the government."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutral Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(0)['message'][1000:1005]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neutral sentiments class are always neutral, they "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(1)['message'][100:105]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pro Sentiment class are mostly Climate Change Activist, they have a do die altitude in respect to governmental involvement on climate changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(2)['message'][100:105]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Factual News sentiment class are mostly inquisitive, they like to get factual news consigning climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6rPNhz7LIr5"
   },
   "source": [
    "### Key Insights\n",
    "\n",
    " A- Message Column contains stopwords and, unimportant words like:\n",
    "\n",
    "     1. RT,\n",
    "     2. #,\n",
    "     3. @\n",
    "     4. https: links\n",
    "     5. emoji's \n",
    "     6. ( )\n",
    "     7. +\n",
    "     8. !~ \"\" \n",
    "\n",
    " B- The Dataset Comprises of Unbalanced Data\n",
    "\n",
    " C- Giving that \"tweetid\" comprises of unique values, it will be   non-relevant in the analysis, therefore might not be used for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWqSc11kLIr5"
   },
   "source": [
    "Having derived certain insights from the data, the next step is to perform data engineering...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGbyA__jLIr6"
   },
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given that our datasets contains a non_numerical column certain preprocessing steps must be carried out, which involves:\n",
    "\n",
    "    . Letter casing: Converting all letters to either upper case or lower case.\n",
    "    . Tokenizing: Turning the tweets into tokens. Tokens are words separated by spaces in a text.\n",
    "    . Noise removal: Eliminating unwanted characters, such as HTML tags, punctuation marks, special characters, white spaces etc.\n",
    "    . Stopwords removal: Some words do not contribute much to the machine learning model, so it's good to remove them. A list of stopwords can be defined by the nltk library, or it can be business-specific.\n",
    "    . Stemming: Eliminating affixes (circumfixes, suffixes, prefixes, infixes) from a word in order to obtain a word stem. Porter Stemmer is the most widely used technique because it is very fast. Generally, stemming chops off end of the word, and mostly it works fine.\n",
    "    OR\n",
    "    . Lemmatization: the process of reducing the different forms of a word to one single form, for example, reducing \"builds\", \"building\", or \"built\" to the lemma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!! Before we begin any preprocessing steps, lets recall both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.head(), test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let just have a view of the rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets now preprocess the dataset, by creating a function that cleans, transforms and lemmatize the words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"This function takes in pandas dataframe, removes URL hyperlinks, stopwords, punctuation noises, and lemmatize the text.\"\"\"\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    point_noise = string.punctuation + '0123456789'\n",
    "    \n",
    "    cleanText = re.sub(r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', \"\", text) #Removing URLs...\n",
    "    cleanText = re.sub(r'@[a-zA-Z0-9\\_\\w]+', '', cleanText)#Remove @mentions\n",
    "    cleanText = re.sub(r'#[a-zA-Z0-9]+', '', cleanText) #Remove '#' symbols\n",
    "    cleanText = re.sub(r'RT', '', cleanText)#Remove RT from text\n",
    "    cleanText = cleanText.lower() #Lowering case\n",
    "    cleanText = re.sub(r'([https][http][htt][th][ht])', \"\",cleanText)\n",
    "    cleanText = ''.join([word for word in cleanText if word not in point_noise]) #Removing punctuations and numbers.\n",
    "    cleanText = \"\".join(word for word in cleanText if ord(word)<128) #Removing NonAscii\n",
    "    cleanText = tokenizer.tokenize(cleanText) #Coverting each words to tokens\n",
    "    cleanText = [lemmatizer.lemmatize(word) for word in cleanText if word not in stopwords_list] #Lemmatizing and removing stopwords\n",
    "    cleanText = [word for word in cleanText if len(word) >= 2]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "    #return cleanText\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the Data Engineering phase we will be working with both Training and Test dataset at the same time.\n",
    "\n",
    "Applying the preprocess function to both training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].apply(preprocess)\n",
    "test_df['message'] = test_df['message'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a view of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in train_df['message'][5000:5010]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking Good....\n",
    "Now lets move on to the next step which is vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Vectorization: Vectorization is the process to convert tokens to numbers. It is an important step because machine learning algorithm works well with numbers and not text.\n",
    "    - In this guide, we will implement vectorization using tf-idf, because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words.\n",
    "\n",
    "    - N-gram is one of the methods in language modeling where text documents can be divided into a combination of sequential words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(ngram_range=(1,20), min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning the vector to include only tokens that appears more than 2 times, with an N_gram range of 20 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform the text using the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = vector.fit_transform(train_df['message'])\n",
    "test_features = vector.transform(test_df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow.. We now have 56943 features for use to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets have a pictorial view of the Vectorized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(train_features.todense(), columns=vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing tfidf DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job so far, we can now derive the dependent and independent variables from the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataset\n",
    "\n",
    "X = train_features #Independent Variables also known as Features\n",
    "y = train_df['sentiment'] #Dependent Variable also known as Target\n",
    "\n",
    "#Test dataset\n",
    "\n",
    "Text_X = test_features #Testing Features\n",
    "tweetid = test_df['tweetid'] #Index tweetid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, lets split the Training dataset into 80% train and 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X, \n",
    "                                                   y, \n",
    "                                                   test_size=0.2, \n",
    "                                                   shuffle=True,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we split our training dataset into 80% train and 20% test as per best practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step will be to solve our data imbalanced issue we noticed while performing EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try solving our imbalance data problem using Synthetic Minority Oversampling Techniques (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE is an oversampling technique that generates synthetic samples from the minority class. It is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instantiate smote as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit smote on the splitted train dataset only, we dont want to be biased when predicting the accuracy of the model by specifying falsified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright lets now see the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before SMOTE:', Counter(y_train))\n",
    "print('After SMOTE:', Counter(y_train_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before SMOTE the Sentiment Classes data where not equally distributed, but with SMOTE all classes as a similar distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we are required to create one or more regression models that are able to accurately predict the Individual Beliefs on Climate Changes. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the modelling works begins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these we will train our model with both the Imbalanced dataset and Balanced dataset, so as to compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions of Individual Beliefs on Climate Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will be training  the below models using our already processed data.\n",
    "            1. Logistic Regression\n",
    "            2. Random Forest Classifier\n",
    "            3. Naive Bayes CLassifier using MultinomialNB\n",
    "            4. Support Vector Machine\n",
    "            5. KNeighbors Classifier.\n",
    "\n",
    "##### Lets now train and compare the above models to see the one with the best fit for predictions using their default parameters.\n",
    "\n",
    "But first lets give a brief overview of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression - is similar to linear regression except, logistic regression predicts weather something is True or False, instead of predicting continuous variables. Also, instead of fitting a line to the data, logistic regression fits an 'S' shaped 'logistic function'.\n",
    "The curve goes from zero (0) to one (1), and that means that the curve tells you the probability of the 'target' based on the feature.\n",
    "\n",
    "2. Random Forest Classifier: are made out of decision trees, is a collection of multiple random decision tress, and it much less sensitive to the training data.\n",
    "\n",
    "3. Multinomial Naïve Bayes classifier: is the probabilistic classification method where Probability of a class given in a document depends on the prior probability of features appeared in the class.\n",
    "\n",
    "4. Support Vector Machine: is a linear model for classification problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "\n",
    "5. KNeighbors Classifier: A super Simple way to classify data. The K in the name of this classifier represents the k nearest neighbors, where k is an integer value specified by the user. Hence as the name suggests, this classifier implements learning based on the k nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give names to the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['LogisticRegression',  'ForestClassifier', 'NaiveBayes', 'LinearSVM', 'KNNClassifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    LogisticRegression(C=10),\n",
    "    RandomForestClassifier(criterion='entropy'),\n",
    "    MultinomialNB(alpha=1),\n",
    "    LinearSVC(C=10, class_weight=None),\n",
    "    KNeighborsClassifier(n_neighbors=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay... we can finally train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model with balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "models = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print('Fitting {:s} model.....'.format(name))\n",
    "    # We train each model using .fit\n",
    "    clf.fit(X_train_sm, y_train_sm)\n",
    "    \n",
    "    \n",
    "    #Predict the Target Class with the below code using .predict\n",
    "    print('..... Predicting')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    train_pred = clf.predict(X_train_sm)\n",
    "    \n",
    "    #Score the models with the accuracy and F1_score.\n",
    "    print('..... Scoring')\n",
    "    accuracy = accuracy_score(y_test, y_pred) # Test Accuracy\n",
    "    f1_Score = f1_score(y_test, y_pred, average='macro')\n",
    "    train_f1_Score = f1_score(y_train_sm, train_pred, average='macro')\n",
    "    \n",
    "    #Save the results to dictionaries\n",
    "    models[name] = clf\n",
    "    \n",
    "    results.append([name, accuracy, f1_Score, train_f1_Score])\n",
    "    \n",
    "#Creating a df from the results    \n",
    "results = pd.DataFrame(results, columns=['Classifier', 'Accuracy Score', 'F1 Score', 'Train F1 Score'])\n",
    "results.set_index('Classifier', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models with imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_results = []\n",
    "imbalanced_models = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print('Fitting {:s} model.....'.format(name))\n",
    "    # We train each model using .fit\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    #Predict the Target Class with the below code using .predict\n",
    "    print('..... Predicting')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    \n",
    "    #Score the models with the accuracy and F1_score.\n",
    "    print('..... Scoring')\n",
    "    accuracy = accuracy_score(y_test, y_pred) # Test Accuracy\n",
    "    f1_Score = f1_score(y_test, y_pred, average='macro')\n",
    "    train_f1_Score = f1_score(y_train, train_pred, average='macro')\n",
    "    \n",
    "    #Save the results to dictionaries\n",
    "    imbalanced_models[name] = clf\n",
    "    \n",
    "    imbalanced_results.append([name, accuracy, f1_Score, train_f1_Score])\n",
    "    \n",
    "#Creating a df from the results    \n",
    "imbalanced_results = pd.DataFrame(imbalanced_results, columns=['Classifier', 'Accuracy Score', 'F1 Score', 'Train F1 Score'])\n",
    "imbalanced_results.set_index('Classifier', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost Done, but first lets check the performance of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1_score will be the basic metric in scaling the performance of our trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Data Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('F1 Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced Data Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalanced_results.sort_values('F1 Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome: We can see that their is little difference in the result but the balanced dataset has a more better overall prediction..\n",
    "\n",
    "The Imbalanced dataset has more over fitting problem, because, the Train F1 Score is higher than the balanced dataset Train F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore will be Using the Balanced dataset for further predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Visualize the results of Trained models with balanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize =(15,5))\n",
    "results.sort_values('F1 Score', ascending=False, inplace=True)\n",
    "results.plot(y='F1 Score', kind='bar', ax=ax[0])\n",
    "results.plot(y='Accuracy Score', kind='bar', ax=ax[1])\n",
    "plt.title('Balanced Data Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. in terms of F1_Score,  the logistic Regression seems to be the best, so lets try to do some parameters tunning on the both Logistic Regression and LinearSVM to see if we could get a better F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now Tune the SVMLinear Model and LogisticRegression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'C':[40,20,10,5],\n",
    "    'class_weight': [None]\n",
    "}\n",
    "\n",
    "gridS = GridSearchCV(models['LinearSVM'], param_grid=param_dict, cv=3, n_jobs=1)\n",
    "gridS.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "param_dict = {\n",
    "    'C':[250,200,150,100]\n",
    "}\n",
    "\n",
    "gridL = GridSearchCV(models['LogisticRegression'], param_grid=param_dict, cv=3, n_jobs=1)\n",
    "gridL.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the Best Parameters and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridS.best_params_, gridS.best_score_)\n",
    "print(gridL.best_params_, gridL.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that the LogisticRegression model is still the best with a score of 0.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets retrain the LogisticRegression Model with our best parameters to see the Result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=150)\n",
    "LR.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!, lets get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show Our Predictions using the classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets see the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We Create Functions that will add Ploting our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred), labels,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Values in the Diagonal are the True Positive. TP (Anti:133, Neutral:190, Pro:1408, News:503)\n",
    "False Negatives FN (Anti:145, Neutral:235, Pro:347, News:203)\n",
    "False Positives FP (Anti:103, Neutral:215, Pro:425, News:188)\n",
    "True Negatives TN (Anti:2783, Neutral:2525, Pro:984, News:2270)\n",
    "\n",
    "Giving the result, Pro and News class are well predicted by the Logistic regression model compared to that of Anti and Neutral class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries for the data we want to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"random_state\": 42,\n",
    "          \"model_type\": \"logreg\",\n",
    "          \"vectorizer\": \"tfidf_Vectorizer\",\n",
    "          \"imbalanced\": \"smote\",\n",
    "          \"param_grid\": str(param_dict),\n",
    "          \"stratify\": True\n",
    "          }\n",
    "metrics = {\"f1\": f1,\n",
    "           \"recall\": recall,\n",
    "           \"precision\": precision\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log our parameters and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets explain how the Logistic Regression works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model, makes use of a logistic function: f(x), that takes the shape of a S-Shape Curve known as SIGMOID, the logistic regression model takes the training data and maps the independent variables (features) to exist only between the multi class dependent variables (target) which are (Anti:-1), (Neutral:0), (Pro:1), (News:2). It selects a reference class and uses the one vs rest (OvR) algorithm to predict the probability of a tweet belonging to a particular dependent or target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Our best model is the Logistic Regression model, we set the 'C' parameter to 150, and other parameters to their default parameter, in order to apply a regularization to our data in order to reduce over fitting. \n",
    "The C parameter of 200 gave the best fit to the data, with an best accuracy score percentage of 71%. \n",
    "\n",
    "2. Support Vector Machines:We have used LinearSVC algorithm After experimenting with multiple 'C' parameter values, the 'C' parameter value of 10 in Linear SVC has returned best accuracy percentage with 69%.\n",
    "\n",
    "3. In Multinomial Naïve Bayes classifier, we have introduced smoothing parameter alpha with different values, but there little changes to the accuracy of the model. Using Multinomial Naïve Bayes classifier along with alpha value as 1 has returned 64%.\n",
    "\n",
    "4. For Random Forest Classifier, we experimented with the 'gini' and 'entropy' criterion, with the default parameter, without specifying a max_leaf_nodes. This multiple random decision tree returns and accuracy score of 70.2% and F1 Score of 71%.\n",
    "\n",
    "5.  KNeighbors Classifier: this classifier implements learning based on the 10 nearest neighbors. The classifier is the poorest among all models with an accuracy score of 35.9% and an F1 score of 36.1%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Team2(2110ACDS_TC)_notebook_Team_2 (1).ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "f1bbc1655b927eebf162c370b52077a239090e504a15e5628dbcc94473f432c4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
