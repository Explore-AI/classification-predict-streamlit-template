{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a6350f8",
   "metadata": {},
   "source": [
    "# Super Classy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d12722d0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Climate Change Perception on Twitter using Classification Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78e0e05e",
   "metadata": {},
   "source": [
    "## Team Members \n",
    "- Katlego Thobye\n",
    "- Njabulo Nxumalo\n",
    "- Thato Matlou\n",
    "- Seema Masekwameng\n",
    "- Khuthadzo Mamushiana\n",
    "- Matlala Nyama\n",
    "- Lerato Ntoene"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef9f3ae3",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8657c50",
   "metadata": {},
   "source": [
    "- Introduction\n",
    "    - Predict Overview\n",
    "    - Import Packages\n",
    "    - Loading Data\n",
    "- Loading Data\n",
    "- Exploratory Data Analysis\n",
    "- Feature Engineering\n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4f5ed33",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f94721d",
   "metadata": {},
   "source": [
    "In today's world, where climate change is a pressing global concern, understanding public sentiment towards this issue is of paramount importance for companies seeking to align their marketing strategies with societal perspectives. By accurately gauging public perception and identifying potential threats associated with climate change, companies can develop targeted campaigns that resonate with diverse consumer sentiments and effectively address their concerns. This report presents a comprehensive analysis utilizing Jupyter Notebook to classify the sentiment of tweets related to climate change, providing valuable insights for companies in shaping their future marketing strategies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c88c31c5",
   "metadata": {},
   "source": [
    "## Objective and Mission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80378d88",
   "metadata": {},
   "source": [
    "The primary objective of this project is to deliver a precise and durable solution that enables companies to gain insights into how people perceive climate change and assess whether it is perceived as a real threat. By leveraging cutting-edge classification techniques, our mission is to equip companies with a tool that taps into a wide range of consumer sentiments across various demographics and geographic regions. Through this, we aim to enhance their understanding of public perception and empower them to shape future marketing strategies based on valuable insights gleaned from social media data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "349e7fdc",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c39f7b6",
   "metadata": {},
   "source": [
    "To achieve our objective, we employ classification techniques within the Jupyter Notebook environment. The core methodology revolves around training a classification model using a pre-labeled dataset, which encompasses tweets related to climate change. By leveraging this labeled dataset, we can effectively train our model to predict the sentiment of novel tweet data accurately.\n",
    "\n",
    "Utilizing machine learning algorithms and natural language processing techniques, we develop a robust classification model that can analyze and classify sentiments expressed in tweets. This model takes into account various features of the tweet, such as the text content, contextual information, and linguistic patterns, to make sentiment predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b24896ee",
   "metadata": {},
   "source": [
    "## Initializing Comet Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08969cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "experiment = Experiment(\n",
    "  api_key = \"TmIAhAxCYHRZd3q9SzUZ1tenu\",\n",
    "  project_name = \"twitter-sentiment-classification\",\n",
    "  workspace=\"xuzmocode4-325\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "408bb6c6",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf72d6e8",
   "metadata": {},
   "source": [
    "### Importing Essential Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import matplotlib.style as style # style selection for visuals\n",
    "%matplotlib inline \n",
    "import nltk # natural language processing platform\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re # regular expressions for text processing\n",
    "import string # python library for string processing\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39771935",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "894cb8b9",
   "metadata": {},
   "source": [
    "### Setting Up Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('Solarize_Light2')\n",
    "# standard dimensions for visuals\n",
    "l = 11 #visual length   \n",
    "w = 6 #visual width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b49317fb",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4137653",
   "metadata": {},
   "source": [
    "Our data set consists of two features and a label. The main feature is the `message` column that contains a tweet related to global warming. The label `sentiment` catagorizes tweets according to four classes, namely `news`, `neutral`, `pro` and `anti`. Our aim will be to create a machine learning model that will be able to acurately classify any tweet according to these four buckets based on the textual message data of a tween only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test_with_no_labels.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeee2423",
   "metadata": {},
   "source": [
    "There are `15819 categorized observations` in the train dataset and `10546 uncategorized observations` in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", df_train.shape[0], \"categorized observations.\")\n",
    "print(\"There are\", df_test.shape[0], \"uncategorized observations.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb15f75",
   "metadata": {},
   "source": [
    "There are no null values in either the train or test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values\n",
    "print(\"There are\", df_train.isnull().sum().sum(), \"null values in the train dataset.\") \n",
    "print(\"There are\", df_test.isnull().sum().sum(), \"null values in the test dataset.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e38cc39",
   "metadata": {},
   "source": [
    "The `tweetid` feature simply uniquely identifies each tweet and most probably will add no real value in classification machine model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tweetid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e754c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proxy = df_train.copy()\n",
    "train_proxy.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c799bdf3",
   "metadata": {},
   "source": [
    "In order to analyze the length of tweets, we created a new feature called `size` which is a count of the `number of characters per tweet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [len(tweet) for tweet in df_train['message']]\n",
    "train_proxy['size'] = size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9cb3cb7",
   "metadata": {},
   "source": [
    "The tweets range from `14` to `208` characters in length. The average size of a tweet is about `124` characters long. Most tweets are `140` characters in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411649bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shortest tweet is\", train_proxy['size'].min(), \"characters in length.\")\n",
    "print(\"The longest tweet is\", train_proxy['size'].max(), \"characters in length.\")\n",
    "print(\"Most tweets are\", train_proxy['size'].mode()[0], \"characters in length.\")\n",
    "print(\"The average tweets is\", round(train_proxy['size'].mean()), \"characters in length.\")\n",
    "print(\"The median tweet is\", round(train_proxy['size'].median()), \"characters in length.\")\n",
    "train_proxy['size'].describe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bf8d1cb",
   "metadata": {},
   "source": [
    "### Visualizing Analysis Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76de7131",
   "metadata": {},
   "source": [
    "#### Tweet Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(l,w))\n",
    "plt.hist(train_proxy['size'])\n",
    "plt.title(\"Distribution of Tweet Lengths\")\n",
    "plt.xlabel(\"Length of Tweet In Charaters\") #X-label of the data\n",
    "plt.ylabel(\"Number of Tweets\")      #Y_label of the data\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3a61319",
   "metadata": {},
   "source": [
    "#### Box Plots on Tweet Length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d9bb93e",
   "metadata": {},
   "source": [
    "Below we attempt to visualize the `5 number summary` of each category of tweet as well as the dataset as a whole using `box and whiskers` diagrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating class subsets for the datase\n",
    "\n",
    "df_anti = train_proxy.copy()[df_train['sentiment'] == -1]\n",
    "df_neutral = train_proxy.copy()[df_train['sentiment'] == 0]\n",
    "df_pro = train_proxy.copy()[df_train['sentiment'] == 1]\n",
    "df_news = train_proxy.copy()[df_train['sentiment'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053684d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the size data in separate variables\n",
    "\n",
    "pro_len = df_pro['size']\n",
    "neutral_len = df_neutral['size']\n",
    "anti_len = df_anti['size']\n",
    "news_len = df_news['size']\n",
    "data_len = train_proxy['size']\n",
    "\n",
    "#creating a list of all the length datasets\n",
    "\n",
    "len_data = [pro_len, anti_len, neutral_len, news_len, data_len]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(l,w))\n",
    "\n",
    "# Create the box plots\n",
    "ax.boxplot(len_data, vert=False)\n",
    "\n",
    "# Set the labels for each box plot\n",
    "labels = ['pro', 'anti', 'neutral', 'news', 'main data']\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('Box and Whiskers Diagram For Tweet Lengths Per Category')\n",
    "plt.xlabel('Length In Characters')\n",
    "plt.ylabel('Class')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f232ef2",
   "metadata": {},
   "source": [
    "#### Raw Text Wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa849d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\" \".join(i for i in df_train[\"message\"])\n",
    "text = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud()\n",
    "tweet_cloud = wordcloud.generate(text)\n",
    "plt.figure( figsize=(l,w))\n",
    "plt.imshow(tweet_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6025469",
   "metadata": {},
   "source": [
    "In the raw text `wordcloud` diagram we see many words not related to natural human language, the most prominent being `https` which in a tweet context is probably associated with external links to websites on the internet beyond Twitter. In order to only make predictions based on the texts related to natural language the message feature must be cleaned so that only those parts of a tweet remain. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a01a3dc",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "nltk.download('vader_lexicon')\n",
    "from wordcloud import WordCloud,ImageColorGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c80c6c77",
   "metadata": {},
   "source": [
    "In order to process the tweet messages more effectively the tweets are cleaned using the `clean` function defined in the code cell below.\n",
    "The clean function does the following.\n",
    "- Converts all tweet text to lowercase.\n",
    "- Removes URLs.\n",
    "- Removes punctuation.\n",
    "- Removes numbers.\n",
    "- Removes stopwords.\n",
    "- Removes line-break code syntax.  \n",
    "- Reduce each word that is not a stop word to a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47104be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add comments per line\n",
    "stopword=set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower() \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proxy['clean'] = train_proxy['message'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proxy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proxy.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c473a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean'] = df_train['message'].apply(clean)\n",
    "df_test['clean'] = df_test['message'].apply(clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8db6778c",
   "metadata": {},
   "source": [
    "#### Clean Text Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(i for i in train_proxy[\"clean\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilinear\n",
    "wordcloud = WordCloud()\n",
    "tweet_cloud = wordcloud.generate(text)\n",
    "plt.figure( figsize=(l,w))\n",
    "plt.imshow(tweet_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19775684",
   "metadata": {},
   "source": [
    "The clean text `Word Cloud`, shows words in and phrases in natural language that commonly appear in tweets related to global warming and climate change.\n",
    "The top three phrases seem to be:\n",
    "\n",
    "- Climate Change\n",
    "- Global Warming\n",
    "- RT\n",
    "- Change RT \n",
    "- Believe Climate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d60f3ec",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08de9942",
   "metadata": {},
   "source": [
    "#### Pie Chart for Retweet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_rt = lambda x: 'rt' in x\n",
    "train_proxy['rt'] = [1 if has_rt(i) else 0 for i in train_proxy['clean']]\n",
    "train_proxy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_counts = train_proxy[\"rt\"].value_counts()\n",
    "rt_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_counts = train_proxy[\"rt\"].value_counts()\n",
    "plt.figure( figsize=(l,w))\n",
    "plt.pie(rt_counts, labels=[\"Has RT\", \"Has No RT\"], explode=[0.05,0], autopct='%1.1f%%')\n",
    "plt.title(\"Pie Chart of Percentage Tweets with 'RT' Vs Without\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d299f7d",
   "metadata": {},
   "source": [
    "Most tweets have an `rt` which means it is highly likely if not certain that they are `Retweets`.\n",
    "\n",
    "According to [twitter.com](https://help.twitter.com/en/using-twitter/retweet-faqs#:~:text=Twitter's%20Retweet%20feature%20helps%20you,re%2Dposting%20someone%20else's%20content.):\n",
    " \n",
    "'A Retweet is a re-posting of a Tweet. Twitter's Retweet feature helps you and others quickly share that Tweet with all of your followers. You can Retweet your own Tweets or Tweets from someone else. Sometimes people type \"RT\" at the beginning of a Tweet to indicate that they are re-posting someone else's content. This isn't an official Twitter command or feature, but signifies that they are quoting another person's Tweet.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02ef144f",
   "metadata": {},
   "source": [
    "#### Sentiment Distribution Bar Plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b67f30b8",
   "metadata": {},
   "source": [
    "Plotting the distribution of classes in our dataset we see that most tweets are `Pro` (display belief in) global warming and climate change (more than twice any other class). Other than that, alot of the tweets are `News` related. A fewer amount of the tweets are `Neutral` and the least amount of tweets are `Anti` (show little or no signs of belief in) global warming or climate change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a barplot for the train dataset classes\n",
    "senti_counts = df_train[\"sentiment\"].value_counts()\n",
    "news = senti_counts[2] \n",
    "pro = senti_counts[1]   \n",
    "neutral = senti_counts[0]\n",
    "anti = senti_counts[-1]  \n",
    "\n",
    "plt.figure( figsize=(l,w))\n",
    "plt.barh(['News ','Pro','Neutral','Anti'], [news,pro,neutral,anti]) #Use matplotlib horizontal bar graph to compare classes of tweets.\n",
    "plt.xlabel('Count of Tweets') #X-label of the data\n",
    "plt.ylabel('Class') #Y_label of the data \n",
    "plt.title('Distribution of Classes In The Dataset') #Give the data a title 'Dataset lables distribution'\n",
    "plt.show() ##Display the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27abb47a",
   "metadata": {},
   "source": [
    "#### Unique Words Counts and Frequency Plot Diagram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "938b7839",
   "metadata": {},
   "source": [
    "Counting how many uniqe words there are in the text body that is the summation of all words in each tweet of the data set will assist in deciding the maximum and minimum threshold parameters for the vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = {}\n",
    "words = text.split(\" \")\n",
    "for word in words:\n",
    "    if word != \" \" and word !=\"\":\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary by values and get the top 20 items\n",
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x:x[1], reverse=True)[:30]\n",
    "top_30_words = dict(sorted_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the x-labels and values from the top 20 data\n",
    "x_labels = list(top_30_words.keys())\n",
    "values = list(top_30_words.values())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(l,w))\n",
    "\n",
    "# Plot the data\n",
    "ax.bar(x_labels, values)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Top 30 Most Used Words')\n",
    "\n",
    "# Rotate the x-labels if needed\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum([word for word in freq_dict.values()])\n",
    "print(\"The dataset has\", total_words, \"unique words in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_under10 =  [word for word in freq_dict.values() if word < 10]\n",
    "fig, ax = plt.subplots(figsize=(l,w))\n",
    "plt.hist(words_under10, bins=10)\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word frequency\")\n",
    "plt.show()\n",
    "print(\"There are\", sum(words_under10), \"in the dataset that appear below 10 times.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "672f36c8",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a27f2b",
   "metadata": {},
   "source": [
    "#### Setting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dffa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSConst = 144\n",
    "low_lim = 5\n",
    "upper_lim = 3000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad640abb",
   "metadata": {},
   "source": [
    "#### Importing Essential Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, StratifiedKFold, RepeatedStratifiedKFold\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ad5c33",
   "metadata": {},
   "source": [
    "#### Importing Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521523dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = ['RidgeClassifier','LogisticRegression','RandomForestClassifier', \n",
    "        'LinearSVC', 'BernoulliNB', ]\n",
    "# Storing models in a list for evaluation\n",
    "method = [RidgeClassifier(random_state=RSConst),\n",
    "              LogisticRegression(multi_class='ovr', max_iter=90000, solver='saga', random_state=RSConst),\n",
    "              RandomForestClassifier(bootstrap=True), svm.LinearSVC(multi_class='ovr', max_iter=90000), \n",
    "              BernoulliNB()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0a54188",
   "metadata": {},
   "source": [
    "#### Importing Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc76aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb7701c8",
   "metadata": {},
   "source": [
    "#### Preparing The Data For Model Training [Non-Resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9427838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_tester(X_train, y_train, X_test, y_test, clf_name, clf_type):\n",
    "    model_stats = {}  # Initialize dictionary to store performance statistics\n",
    "\n",
    "    for name, clf in zip(clf_name, clf_type):\n",
    "\n",
    "        model = Pipeline([('vect', TfidfVectorizer(min_df=5, max_df=2000, ngram_range=(1, 3))),\n",
    "                          ('std', StandardScaler(with_mean=False)),\n",
    "                          ('clf', clf)])\n",
    "\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        result = %timeit -q -o model.fit(X_train, y_train) # Logging a runtime for each model\n",
    "        model_pred = model.predict(X_test)  # Make predictions on the testing data\n",
    "\n",
    "        # Compute performance metrics and store in model_stats dictionary\n",
    "        model_stats[name] = {\n",
    "            'F1-Macro': metrics.f1_score(y_test, model_pred, average='macro'),\n",
    "            'F1-Accuracy': metrics.f1_score(y_test, model_pred, average='micro'),\n",
    "            'F1-Weighted': metrics.f1_score(y_test, model_pred, average='weighted'),\n",
    "            'Run-Time': result.best\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame.from_dict(model_stats, orient='index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0166b8c",
   "metadata": {},
   "source": [
    "#### Preparing The Data For Model Training [Resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f42a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlen = 3000\n",
    "news_resampled = resample(df_news, random_state=RSConst, n_samples = rlen) # reproducible results\n",
    "pro_resampled = resample(df_pro, random_state=RSConst, n_samples = rlen) # reproducible results\n",
    "anti_resampled = resample(df_anti, random_state=RSConst, n_samples = rlen) # reproducible results\n",
    "neutral_resampled = resample(df_neutral, random_state=RSConst, n_samples = rlen) # reproducible results\n",
    "\n",
    "resampled = pd.concat([news_resampled, pro_resampled , anti_resampled, neutral_resampled])\n",
    "resampled['sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36917a7e",
   "metadata": {},
   "source": [
    "resampled. head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05511ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal bar graph to compare samples of tweets per class.\n",
    "plt.figure( figsize=(l,w))\n",
    "len_unsampled = [news, pro, anti, neutral]\n",
    "resampled_len = [rlen for l in len_unsampled] \n",
    "plt.bar(['News ','Pro','Neutral','Anti'], [news,pro,neutral,anti])\n",
    "plt.bar(['News ','Pro','Neutral','Anti'], resampled_len, color='green')\n",
    "\n",
    "plt.xlabel('Count of Tweets') #X-label of the data\n",
    "plt.ylabel('# of observations') #Y_label of the data \n",
    "plt.title('Distribution of Class Sample Sizes In The Dataset') #Give the data a title 'Dataset lables distribution'\n",
    "plt.legend(['resampled','original'])\n",
    "plt.show() ##Display the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled['clean'] = resampled['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = resampled['clean']\n",
    "y = resampled['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train , y_test = train_test_split(X , y,\n",
    "                                                       test_size =0.2, \n",
    "                                                       random_state=RSConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = performance_tester(X_train, y_train, X_test, y_test, id, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values('F1-Weighted', ascending=False)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b27fa2e9",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abe955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning(*args , **kwargs):\n",
    "  best_params = {}\n",
    "\n",
    "  for clf in method:\n",
    "    model = Pipeline([('vect', TfidfVectorizer(min_df=5, max_df=2000, ngram_range=(1, 3))),\n",
    "                          ('std', StandardScaler(with_mean=False)),\n",
    "                          ('clf', clf)])\n",
    "    \n",
    "    model.fit(X_train, y_train) #Training\n",
    "    \n",
    "    #Get models performing parameters\n",
    "    params = model.get_params()\n",
    "    model_name = clf.__class__.__name__ \n",
    "    model_name = {}\n",
    "    for key in params:\n",
    "      if key.startswith(\"clf\"):\n",
    "        if len(key) < 5:\n",
    "          model_name['model'] = params[key]\n",
    "        else:\n",
    "            model_name[key[5:]] = params[key]\n",
    "    best_params[clf.__class__.__name__] = model_name\n",
    "  return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c41178",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = param_tuning(method, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08179c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e977acb2",
   "metadata": {},
   "source": [
    "### Conducting Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52c69985",
   "metadata": {},
   "source": [
    "Setting Up `evaluate` Function for Comet Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "  return {\n",
    "      'f1': f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "      'precision': precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "      'recall': recall_score(y_test, y_pred, average=\"weighted\")\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "780a1580",
   "metadata": {},
   "source": [
    "#### Instantiating The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_vect = Pipeline([('vect', TfidfVectorizer(max_df=2000, min_df=5, ngram_range=(1, 3))),\n",
    "                          ('scaler', StandardScaler(with_mean=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ebba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = resampled['clean']\n",
    "y = resampled['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train , y_test = train_test_split(X, y,\n",
    "                                                       test_size =0.4, \n",
    "                                                       random_state=RSConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_vect.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"../resources/scale_vect.pkl\" #pickling the count vectorizer\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(scale_vect,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"../resources/scale_vect.pkl\"\n",
    "with open(model_load_path,'rb') as file: \n",
    "    scale_vect_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale_vect_pkl.transform(X_train)\n",
    "X_test = scale_vect_pkl.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31e390be",
   "metadata": {},
   "source": [
    "#### Setting Up Cross Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RSConst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71bd660",
   "metadata": {},
   "source": [
    "##### Model 1: `RidgeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[id[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a93e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = comet_ml.Experiment(\"TmIAhAxCYHRZd3q9SzUZ1tenu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RidgeClassifier(random_state=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = list(np.linspace(0.02,1.0,10))\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid_search1 = GridSearchCV(estimator= model1,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=stratified_kfold,\n",
    "                           error_score=0,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae359af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search1.fit(X_train, y_train)\n",
    "prediction1 = grid_search1.predict(X_test)\n",
    "cv_score = grid_search1.best_score_\n",
    "test_score = grid_search1.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93726320",
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.train():\n",
    "  metrics = evaluate(y_test, prediction1)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cross-validation score: {cv_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "grid_search1.best_params_    \n",
    "grid_search1.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f010a06d",
   "metadata": {},
   "source": [
    "##### Model 2: `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[id[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0340c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(max_iter=10000, multi_class='ovr', random_state=144,\n",
    "                    solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = list(np.linspace(1.0,0.01,10))\n",
    "param_grid = dict(C=C)\n",
    "grid_search2 = GridSearchCV(estimator= model2,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=stratified_kfold,\n",
    "                           error_score=0,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2.fit(X_train, y_train)\n",
    "prediction2 = grid_search2.predict(X_test)\n",
    "cv_score = grid_search2.best_score_\n",
    "test_score = grid_search2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.train():\n",
    "  metrics = evaluate(y_test, prediction2)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cross-validation score: {cv_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "grid_search2.best_params_    \n",
    "grid_search2.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71287d19",
   "metadata": {},
   "source": [
    "##### Model 3: `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8150a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[id[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = RandomForestClassifier(min_samples_leaf=5, n_estimators=100, max_depth=15, min_samples_split=10, max_features='sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f377db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_alpha = list(np.linspace(1.0,0.01,10))\n",
    "param_grid = dict(ccp_alpha=alpha)\n",
    "grid_search3 = GridSearchCV(estimator= model3,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=stratified_kfold,\n",
    "                           error_score=0,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search3.fit(X_train, y_train)\n",
    "prediction3 = grid_search3.predict(X_test)\n",
    "cv_score = grid_search3.best_score_\n",
    "test_score = grid_search3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.train():\n",
    "  metrics = evaluate(y_test, prediction3)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cross-validation score: {cv_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "grid_search3.best_params_    \n",
    "grid_search3.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb5c534",
   "metadata": {},
   "source": [
    "##### Model 4:  `Supoort Vector Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[id[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e687a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = list(np.linspace(0.1,0.02,10))\n",
    "param_grid = dict(C=C)\n",
    "grid_search4 = GridSearchCV(estimator= model4,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=stratified_kfold,\n",
    "                           error_score=0,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63940df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search4.fit(X_train, y_train)\n",
    "prediction4 = grid_search4.predict(X_test)\n",
    "cv_score = grid_search4.best_score_\n",
    "test_score = grid_search4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.train():\n",
    "  metrics = evaluate(y_test, prediction4)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f41a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cross-validation score: {cv_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "grid_search4.best_params_    \n",
    "grid_search4.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78d0915b",
   "metadata": {},
   "source": [
    "##### Model 5: `MultinomialNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[id[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cea2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = list(np.linspace(1.0,0.02,10))\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid_search5 = GridSearchCV(estimator= model5,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=stratified_kfold,\n",
    "                           error_score=0,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search5.fit(X_train, y_train)\n",
    "prediction5 = grid_search5.predict(X_test)\n",
    "cv_score = grid_search5.best_score_\n",
    "test_score = grid_search5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.train():\n",
    "  metrics = evaluate(y_test, prediction5)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cross-validation score: {cv_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "grid_search5.best_params_    \n",
    "grid_search5.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f26fd82",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = resampled['clean']\n",
    "y = resampled['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=RSConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45639ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale_vect_pkl.transform(X_train)\n",
    "X_test = scale_vect_pkl.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RidgeClassifier(random_state=144, alpha=0.51)\n",
    "model2 = LogisticRegression(C=0.01, max_iter=10000,\n",
    "                            multi_class='ovr', \n",
    "                            random_state=144,\n",
    "                            solver='saga')\n",
    "model3 = RandomForestClassifier(ccp_alpha=1.0)\n",
    "model4 = svm.SVC(C=0.1)\n",
    "model5 = BernoulliNB(alpha=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d27540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train)\n",
    "model_save_path = \"../resources/model1.pkl\" #pickling the count vectorizer\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model1,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, y_train)\n",
    "model_save_path = \"../resources/model2.pkl\" #pickling the count vectorizer\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model2,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f894dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X_train, y_train)\n",
    "model_save_path = \"../resources/model3.pkl\" #pickling the count vectorizer\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model3,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08320feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.fit(X_train, y_train)\n",
    "model_save_path = \"../resources/model4.pkl\" #pickling the count vectorizer\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model4,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ca7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.fit(X_train, y_train)\n",
    "model_save_path = \"../resources/model5.pkl\" #pickling the count vectorizer\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model5,file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "220a08e8",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93334f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model):\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    categories = ['Pro', 'Anti', 'Neutral', 'News']\n",
    "    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(itertools.cycle(group_names), group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(cf_matrix.shape[0], cf_matrix.shape[1])\n",
    "    \n",
    "    sns.heatmap(cf_matrix, annot=labels, cmap='Blues', fmt='',\n",
    "                xticklabels=categories, yticklabels=categories) #,\n",
    "    plt.xlabel(\"Predicted values\", fontdict={'size': 14}, labelpad=10)\n",
    "    plt.ylabel(\"Actual values\", fontdict={'size': 14}, labelpad=10)\n",
    "    plt.title(\"Confusion Matrix\", fontdict={'size': 18}, pad=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1b1bb3a",
   "metadata": {},
   "source": [
    "#### RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"../resources/model1.pkl\"\n",
    "with open(model_load_path,'rb') as file: \n",
    "    model1_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_preds = model1_pkl.predict(X_test)\n",
    "with experiment.test():\n",
    "  metrics = evaluate(y_test, model1_preds)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{id[0]} Evaluation\")\n",
    "print(classification_report(y_test, model1_preds, target_names=class_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3542b74",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"../resources/model2.pkl\"\n",
    "with open(model_load_path,'rb') as file: \n",
    "    model2_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_preds = model2_pkl.predict(X_test)\n",
    "with experiment.test():\n",
    "  metrics = evaluate(y_test, model2_preds)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6828c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1eabf08",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"../resources/model3.pkl\"\n",
    "with open(model_load_path,'rb') as file: \n",
    "    model3_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97923d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_preds = model3_pkl.predict(X_test)\n",
    "with experiment.test():\n",
    "  metrics = evaluate(y_test, model3_preds)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(model3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219a69ff",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bdc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"../resources/model4.pkl\"\n",
    "with open(model_load_path,'rb') as file: \n",
    "    model4_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_preds = model4_pkl.predict(X_test)\n",
    "with experiment.test():\n",
    "  metrics = evaluate(y_test, model4_preds)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(model4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "396888dc",
   "metadata": {},
   "source": [
    "#### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_path = \"../resources/model5.pkl\"\n",
    "with open(model_load_path,'rb') as file: \n",
    "    model5_pkl = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7075f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_preds = model5.predict(X_test)\n",
    "with experiment.test():\n",
    "  metrics = evaluate(y_test, model5_preds)\n",
    "  experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aaabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{id[4]} Evaluation\")\n",
    "print(classification_report(y_test, model5_preds, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3575e0fd",
   "metadata": {},
   "source": [
    "## Making Predictiona on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10704857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2160f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = df_test['clean']\n",
    "X_pred = scale_vect_pkl.transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed95ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = model1.predict(X_pred)\n",
    "submission = df_test[['tweetid', 'sentiment']]\n",
    "submission.to_csv('submission1.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = model2.predict(X_pred)\n",
    "submission = df_test[['tweetid', 'sentiment']]\n",
    "submission.to_csv('submission2.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bad792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = model3.predict(X_pred)\n",
    "submission = df_test[['tweetid', 'sentiment']]\n",
    "submission.to_csv('submission3.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = model4.predict(X_pred)\n",
    "submission = df_test[['tweetid', 'sentiment']]\n",
    "submission.to_csv('submission4.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sentiment'] = model5.predict(X_pred)\n",
    "submission = df_test[['tweetid', 'sentiment']]\n",
    "submission.to_csv('submission5.csv',index=False)\n",
    "submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "954c3a39",
   "metadata": {},
   "source": [
    "## Results and Benefits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf466453",
   "metadata": {},
   "source": [
    "The results obtained from this sentiment analysis provide valuable insights into the prevailing public sentiment regarding climate change. Companies can leverage these insights to gain a comprehensive understanding of consumer perspectives, identify emerging trends, and detect potential shifts in public sentiment across different demographics and geographic regions.\n",
    "\n",
    "By incorporating these findings into their marketing strategies, companies can tailor their messaging and communication efforts to resonate with target audiences effectively. This data-driven approach empowers companies to stay ahead of the curve and make informed decisions, ensuring their marketing campaigns align with the ever-evolving public sentiment surrounding climate change."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c968dd5",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "034cf805",
   "metadata": {},
   "source": [
    "In conclusion, this report introduces a Jupyter Notebook-based approach to sentiment analysis, focusing specifically on public perception of climate change on Twitter. By leveraging classification techniques, we can accurately predict the sentiment expressed in tweets, enabling companies to tap into valuable consumer sentiments across diverse demographics and geographic regions. This empowers companies to shape future marketing strategies based on data-driven insights and effectively address public concerns surrounding climate change. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cdd1c97",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71f66afe",
   "metadata": {},
   "source": [
    "[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
