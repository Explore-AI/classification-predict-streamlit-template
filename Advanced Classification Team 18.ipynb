{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Advanced Classification Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "We {**TEAM 18**}, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Twitter Sentiment Classification\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "\n",
    "### The evaluation metric\n",
    "Mean F1-Score. The F1 score, commonly used in information retrieval, measures performance using using the statistics precision and recall.\n",
    "\n",
    "Precision is the ratio of true positives to all predicted positives. Recall is the ratio of true positives to all actual positives.\n",
    "The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\n",
    "\n",
    "### Submission Format\n",
    "For every tweet in the dataset, submission files should contain two columns: tweetid and sentiment. sentiment should be a space-delimited list. Every tweetid will have a sentiment, as per your prediction. Refer to the Description page for more information about the valid classes in the sentiment column.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "tweetid,sentiment\n",
    "35326,1\n",
    "15327,-1\n",
    "54232,0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# set plot style\n",
    "sns.set()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import urllib\n",
    "import string\n",
    "from string import printable\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9149b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test_with_no_labels.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436da474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample from the top of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1d0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll print off a list of all the tweetid which are present in this dataset.\n",
    "## tweetid = list(train_data.tweetid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab12c63",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "### Removing Noise\n",
    "\n",
    "In text analytics, removing noise (i.e. unneccesary information) is a key part of getting the data into a usable format. Some techniques are standard, but your own data will require some creative thinking on your part.\n",
    "\n",
    "For the train dataset we will be doing the following steps:\n",
    "\n",
    "- removing the web-urls\n",
    "- making everything lower case\n",
    "- removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc380f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing web-urls and replacing with empty ''\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = ''\n",
    "train_data['message'] = train_data['message'].replace(to_replace = pattern_url, \n",
    "                                            value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e82c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing text before and including the colon : sign\n",
    "train_data['message'] = train_data['message'].replace(r'^.+:', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f0296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words that starts with '@' \n",
    "pattern = r'@\\w+'\n",
    "subs = ''\n",
    "train_data['message'] = train_data['message'].replace(to_replace = pattern, \n",
    "                                            value = subs, regex = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34f0d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words that starts with '#'\n",
    "patt = r'#\\w+'\n",
    "sub = ''\n",
    "train_data['message'] = train_data['message'].replace(to_replace = patt, \n",
    "                                            value = sub, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8da2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming to lower case\n",
    "train_data['message'] = train_data['message'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01184818",
   "metadata": {},
   "source": [
    "#### Remove punctuation\n",
    "\n",
    "- First we make all the text lower case to remove some noise from capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401103ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that removes punctuation from the data frame\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc9a3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['message'] = train_data['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64dca051",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_data['message'].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39645c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a4d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all ascii characters and figures using printable module\n",
    "st = set(printable)\n",
    "train_data['message']= train_data['message'].apply(lambda x: ''.join(\n",
    "    [\"\" if  i not in  st else i for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9695771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bae79e44",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\" (see the Stanford Tokeniser). We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e35bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the message column into tokens \n",
    "tokeniser = TreebankWordTokenizer()\n",
    "train_data['message'] = train_data['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f920a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[researchers, say, we, have, three, years, to,...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016, was, a, pivotal, year, in, the, war, on...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[its, 2016, and, a, racist, sexist, climate, c...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>[how, climate, change, could, be, breaking, up...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, does, trump, actually, believe, about, ...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>[hey, liberals, the, climate, change, crap, is...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>[s, climate, change, equation, in, 4, screensh...</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "0              1  [polyscimajor, epa, chief, doesnt, think, carb...   625221\n",
       "1              1  [its, not, like, we, lack, evidence, of, anthr...   126103\n",
       "2              2  [researchers, say, we, have, three, years, to,...   698562\n",
       "3              1  [2016, was, a, pivotal, year, in, the, war, on...   573736\n",
       "4              1  [its, 2016, and, a, racist, sexist, climate, c...   466954\n",
       "...          ...                                                ...      ...\n",
       "15814          1                                                 []    22001\n",
       "15815          2  [how, climate, change, could, be, breaking, up...    17856\n",
       "15816          0  [what, does, trump, actually, believe, about, ...   384248\n",
       "15817         -1  [hey, liberals, the, climate, change, crap, is...   819732\n",
       "15818          0  [s, climate, change, equation, in, 4, screensh...   806319\n",
       "\n",
       "[15819 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f3d6e",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of transforming to the root word. It uses an algorithm that removes common word-endings from English words, such as “ly,” “es,” “ed,” and “s.”\n",
    "\n",
    "For example, assuming for an analysis you may want to consider “carefully,” “cared,” “cares,” “caringly” as “care” instead of separate words. There are three widely used stemming algorithms, namely:\n",
    "\n",
    "    - Porter\n",
    "    - Lancaster\n",
    "    - Snowball\n",
    "Out of these three, we will be using the SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad632bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to stemm data\n",
    "stemmer = SnowballStemmer('english')\n",
    "def tweet_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33b2f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming the data\n",
    "train_data['message']= train_data['message'].apply(tweet_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bb53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26ce66f",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "A very similar operation to stemming is called lemmatization. Lemmatizing is the process of grouping words of similar meaning together. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebdd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52bb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f9edb3",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Stop words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return a vast amount of unnecessary information. nltk has a corpus of stopwords. Let's print out the stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41a8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to remove stop_words \n",
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95b9ecc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [polyscimajor, epa, chief, doesnt, think, carb...\n",
       "1            [like, lack, evid, anthropogen, global, warm]\n",
       "2        [research, say, three, year, act, climat, chan...\n",
       "3                  [2016, pivot, year, war, climat, chang]\n",
       "4        [2016, racist, sexist, climat, chang, deni, bi...\n",
       "                               ...                        \n",
       "15814                                                   []\n",
       "15815    [climat, chang, could, break, 200millionyearol...\n",
       "15816    [doe, trump, actual, believ, climat, chang, ri...\n",
       "15817    [hey, liber, climat, chang, crap, hoax, tie, c...\n",
       "15818                [climat, chang, equat, 4, screenshot]\n",
       "Name: message, Length: 15819, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['message'] = train_data['message'].apply(remove_stop_words)\n",
    "\n",
    "train_data['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11a841d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[like, lack, evid, anthropogen, global, warm]</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[research, say, three, year, act, climat, chan...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016, pivot, year, war, climat, chang]</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016, racist, sexist, climat, chang, deni, bi...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>[climat, chang, could, break, 200millionyearol...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>[doe, trump, actual, believ, climat, chang, ri...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>[hey, liber, climat, chang, crap, hoax, tie, c...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>[climat, chang, equat, 4, screenshot]</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "0              1  [polyscimajor, epa, chief, doesnt, think, carb...   625221\n",
       "1              1      [like, lack, evid, anthropogen, global, warm]   126103\n",
       "2              2  [research, say, three, year, act, climat, chan...   698562\n",
       "3              1            [2016, pivot, year, war, climat, chang]   573736\n",
       "4              1  [2016, racist, sexist, climat, chang, deni, bi...   466954\n",
       "...          ...                                                ...      ...\n",
       "15814          1                                                 []    22001\n",
       "15815          2  [climat, chang, could, break, 200millionyearol...    17856\n",
       "15816          0  [doe, trump, actual, believ, climat, chang, ri...   384248\n",
       "15817         -1  [hey, liber, climat, chang, crap, hoax, tie, c...   819732\n",
       "15818          0              [climat, chang, equat, 4, screenshot]   806319\n",
       "\n",
       "[15819 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e0c909",
   "metadata": {},
   "source": [
    "## Text feature extraction\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words in the text, indicating the number of times each word has appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43057dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deeea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39006dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775f5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa30ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00169914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7370e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586e986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b978c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b7bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6224f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
